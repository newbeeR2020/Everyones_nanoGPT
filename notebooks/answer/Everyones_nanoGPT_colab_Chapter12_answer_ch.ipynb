{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj6PNj59iqJ8"
      },
      "source": [
        "# **给 Colab 用户的注意事项**\n",
        "\n",
        "# **不要直接在此文件中编辑——你的内容可能会丢失！**\n",
        "\n",
        "# **开始之前请务必先复制一份。**\n",
        "\n",
        "如何复制文件\n",
        "\n",
        "1. 点击左上角的“File”。\n",
        "> *如果找不到菜单栏（比如“File”或“Runtime”），点击右上角的“v”标记展开菜单。*\n",
        "\n",
        "2. 选择“Save a copy in Drive”\n",
        "\n",
        "3. 把复制的文件名改成 “YOURNAMEs\\_FileName.ipynb”。\n",
        "> 例如：如果你叫 Olivia，就改成 Olivias_FileName.ipynb\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* 选中的对号（✅）不会被保存。用 Chrome 的刷新按钮重载页面时，这些标记会消失。<br>\n",
        "如果你想暂停后续做，自己加个文本单元写上“SO FAR DONE”等字样即可。\n",
        "\n",
        "---\n",
        "\n",
        "* 在 Colab 里，**之前的输出会每隔 30 到 90 分钟重置一次**。<br>\n",
        "所以经常会出现 `~~ is not defined` 这类错误。\n",
        "\n",
        "  🔁 遇到 `~~ is not defined` 错误怎么办？\n",
        "  1. 先确认变量拼写无误。<br>\n",
        "  2. 拼写没问题但仍报错，**点击你想重新运行的单元格**。<br>\n",
        "  3. 点击左上角的“Runtime” → 选择“Run before”。<br>\n",
        "    → 这会**重新运行之前的所有单元格**。\n",
        "  4. 再运行该单元格。\n",
        "\n",
        "  如果按照以上步骤还是报错，<br>\n",
        "大概率是之前单元中的 TODO 答案有根本性错误。<br>\n",
        "请检查答案是否正确，<br>\n",
        "或者请教 ChatGPT 或其他编程助手帮你。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXZzMLzXiseX"
      },
      "source": [
        "# **Preparation**\n",
        "\n",
        "本节仅加载之前章节的内容。<br>\n",
        "只需运行代码即可，无需阅读。<br>\n",
        "可以随意跳过。<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s04g6yFiuHE",
        "outputId": "a62bb238-1185-4194-d97f-8cbdcdd1f733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-08 11:25:56--  https://raw.githubusercontent.com/HayatoHongo/nanoGPT_todo/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-07-08 11:25:56 (31.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 下载文件\n",
        "!wget https://raw.githubusercontent.com/HayatoHongo/Everyones_nanoGPT/main/input.txt -O input.txt\n",
        "# 加载刚下载的名为 input.text 的文件，编码为 utf-8。\n",
        "with open(\"input.txt\", 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# 一个用于美观显示张量的函数（可跳过）\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def print_formatted_tensor(*args, width=6, decimals=2):\n",
        "    \"\"\"\n",
        "    A function that neatly formats and displays a PyTorch Tensor, and also prints its size.\n",
        "\n",
        "    Example usage:\n",
        "        print_formatted_tensor(\"name\", tensor)\n",
        "        print_formatted_tensor(tensor)\n",
        "\n",
        "    Args:\n",
        "        *args: If given 1 argument, it is treated as a tensor.\n",
        "               If given 2 arguments, the first is treated as the name, the second as the tensor.\n",
        "        width (int): Display width for each number (default: 6)\n",
        "        decimals (int): Number of decimal places to show (default: 2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine tensor and name from arguments\n",
        "    if not args:\n",
        "        raise ValueError(\"At least one argument is required.\")\n",
        "    if isinstance(args[0], str):\n",
        "        if len(args) < 2:\n",
        "            raise ValueError(\"Tensor is not specified.\")\n",
        "        name, tensor = args[0], args[1]\n",
        "    else:\n",
        "        name, tensor = None, args[0]\n",
        "\n",
        "    # Convert Tensor to List\n",
        "    tensor_list = tensor.detach().cpu().tolist()\n",
        "\n",
        "    def format_list(lst, indent):\n",
        "        \"\"\"Formatting a recursively nested list and returning a string\"\"\"\n",
        "        # If the contents are lists, then re-return\n",
        "        if isinstance(lst, list) and lst and isinstance(lst[0], list):\n",
        "            inner = \",\\n\".join(\" \" * indent + format_list(sub, indent + 2) for sub in lst)\n",
        "            return \"[\\n\" + inner + \"\\n\" + \" \" * (indent - 2) + \"]\"\n",
        "        # For numerical lists\n",
        "        return \"[\" + \", \".join(f\"{v:{width}.{decimals}f}\" for v in lst) + \"]\"\n",
        "\n",
        "    # Formatted string (bar brackets on outermost frames are removed)\n",
        "    formatted = format_list(tensor_list, indent=9)\n",
        "    inner_formatted = formatted[1:-1].strip()\n",
        "\n",
        "    # Result output\n",
        "    if name:\n",
        "        print(name)\n",
        "    print(f\"Tensor Size: {list(tensor.size())}\")\n",
        "    print(\"tensor([\")\n",
        "    print(\" \" * 9 + inner_formatted)\n",
        "    print(\" \" * 7 + \"])\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh_2Ls8tn2rk"
      },
      "source": [
        "# **Chapter 12 Trainer Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEl9A_wk33Pa"
      },
      "source": [
        "### **Section 1: Class Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnILkem04RSW"
      },
      "source": [
        "🔘 **Options**：可能会有一些你用不到的额外选项。\n",
        "\n",
        "`self.model`　`self.optimizer`　`self.data_loader`　`self.config`　`split_data`　`get_batch`　`'train'`、`'val'`　`input_batch`　`target_batch`　`logits`　`self.config.total_training_steps`　`self.config.evaluation_loops`  \n",
        "`loss`　`backward()`　`self.train_step()`　`self.evaluate()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRi90bRasayV"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, data_loader, config):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.data_loader = data_loader\n",
        "        self.config = config\n",
        "\n",
        "    def train_step(self):\n",
        "        # 获取一批用于训练的数据。\n",
        "        input_batch, target_batch = self.data_loader.get_batch('train')\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # 模型前向传播和损失计算\n",
        "        logits, loss = self.model(input_batch, target_batch)\n",
        "        loss.backward()  # 反向传播（误差反向传播）\n",
        "        self.optimizer.step()  # 更新参数\n",
        "\n",
        "        return loss.item() # 返回损失值\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()  # 设置为评估模式\n",
        "        losses = {\"train\": [], \"val\": []} # 计算训练和验证数据的损失\n",
        "        with torch.no_grad():\n",
        "            for split in ['train', 'val']:\n",
        "                for _ in range(self.config.evaluation_loops):\n",
        "                    input_batch, target_batch = self.data_loader.get_batch(split)\n",
        "                    _, loss = self.model(input_batch, target_batch)\n",
        "                    losses[split].append(loss.item())\n",
        "        self.model.train()  # 返回训练模式\n",
        "\n",
        "        # 计算每个拆分（训练集、验证集）的平均损失\n",
        "        return {split: sum(values) / len(values) for split, values in losses.items()}\n",
        "\n",
        "    def train(self):\n",
        "        # 根据配置中指定的次数运行 train_step。\n",
        "        for step in range(self.config.total_training_steps):\n",
        "\n",
        "            # 每隔100次迭代或仅在最后一步进行评估。\n",
        "            if step % self.config.evaluation_frequency == 0 or step == self.config.total_training_steps - 1:\n",
        "                eval_loss = self.evaluate()\n",
        "                print(f\"Step {step}: Train Loss {eval_loss['train']:.4f}, Validation Loss {eval_loss['val']:.4f}\")\n",
        "\n",
        "            # 训练的一步（每次执行的主要过程）\n",
        "            train_loss = self.train_step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "mi8-y1ak4VOS",
        "outputId": "ec0f1c70-6f55-4cf2-fb9f-e5b56297691b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass Trainer:\\n    def __init__(self, model, optimizer, data_loader, config):\\n        self.model = model\\n        self.optimizer = optimizer\\n        self.data_loader = data_loader\\n        self.config = config\\n\\n    def train_step(self):\\n        # Get a batch for training.\\n        input_batch, target_batch = ___________._______(_____)\\n        self.optimizer.zero_grad()\\n\\n        # Model forward pass and loss calculation\\n        logits, loss = _______(_________, __________)\\n        _____.__________  # Backpropagation (Error backpropagation)\\n        self.optimizer.step()  # Update parameters\\n\\n        return loss.item() # Returns the value of the loss\\n\\n    def evaluate(self):\\n        self.model.eval()  # Set to evaluation mode\\n        losses = {\"train\": [], \"val\": []} # Calculate losses on both training and validation data\\n        with torch.no_grad():\\n            for split in [\\'train\\', \\'val\\']:\\n                for _ in range(self.config.evaluation_loops):\\n                    input_batch, target_batch = self.data_loader.get_batch(split)\\n                    _, loss = self.model(input_batch, target_batch)\\n                    losses[split].append(loss.item())\\n        self.model.train()  # Return to training mode\\n\\n        # Calculate the average losses for each split (train, val)\\n        return {split: sum(values) / len(values) for split, values in losses.items()}\\n\\n    def train(self):\\n        # Run train_step the number of times specified in config.\\n        for step in range(_________________________):\\n\\n            # Evaluate every 100 iterations or just at the final step.\\n            if step % self.config.evaluation_frequency == 0 or step == self.config.total_training_steps - 1:\\n                eval_loss = self.evaluate()\\n                print(f\"Step {step}: Train Loss {eval_loss[\\'train\\']:.4f}, Validation Loss {eval_loss[\\'val\\']:.4f}\")\\n\\n            # One step of training (the main process that you do every time)\\n            train_loss = _____________\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "将英文注释、文档字符串和 print 语句简洁、友好、自然地翻译成简体中文。\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, data_loader, config):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.data_loader = data_loader\n",
        "        self.config = config\n",
        "\n",
        "    def train_step(self):\n",
        "        # 获取一批用于训练的数据。\n",
        "        input_batch, target_batch = ___________._______(_____)\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # 模型前向传递和损失计算\n",
        "        logits, loss = _______(_________, __________)\n",
        "        _____.__________  # 反向传播（误差反向传播）\n",
        "        self.optimizer.step()  # 更新参数\n",
        "\n",
        "        return loss.item() # 返回损失值\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()  # 设置为评估模式\n",
        "        losses = {\"train\": [], \"val\": []} # 计算训练和验证数据的损失\n",
        "        with torch.no_grad():\n",
        "            for split in ['train', 'val']:\n",
        "                for _ in range(self.config.evaluation_loops):\n",
        "                    input_batch, target_batch = self.data_loader.get_batch(split)\n",
        "                    _, loss = self.model(input_batch, target_batch)\n",
        "                    losses[split].append(loss.item())\n",
        "        self.model.train()  # 返回训练模式\n",
        "\n",
        "        # 计算每个划分（训练集、验证集）的平均损失\n",
        "        return {split: sum(values) / len(values) for split, values in losses.items()}\n",
        "\n",
        "    def train(self):\n",
        "        # 根据配置中指定的次数运行 train_step。\n",
        "        for step in range(_________________________):\n",
        "\n",
        "            # 每100次迭代评估一次，或仅在最后一步评估。\n",
        "            if step % self.config.evaluation_frequency == 0 or step == self.config.total_training_steps - 1:\n",
        "                eval_loss = self.evaluate()\n",
        "                print(f\"Step {step}: Train Loss {eval_loss['train']:.4f}, Validation Loss {eval_loss['val']:.4f}\")\n",
        "\n",
        "            # 训练的一步（你每次执行的主要过程）\n",
        "            train_loss = _____________\n",
        "\"\"\"\n",
        "将英文注释、文档字符串和 print 语句简洁友好地翻译成简体中文。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUW7ceEA4ek"
      },
      "source": [
        "<details>\n",
        "<summary>点击展开/隐藏答案</summary>\n",
        "\n",
        "```python\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, data_loader, config):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.data_loader = data_loader\n",
        "        self.config = config\n",
        "\n",
        "    def train_step(self):\n",
        "        # 获取一批训练数据\n",
        "        input_batch, target_batch = self.data_loader.get_batch('train')\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # 模型前向计算和损失计算\n",
        "        logits, loss = self.model(input_batch, target_batch)\n",
        "        loss.backward()  # 反向传播（误差回传）\n",
        "        self.optimizer.step()  # 参数更新\n",
        "\n",
        "        return loss.item()  # 返回损失值\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.model.eval()  # 切换到评估模式\n",
        "        losses = {\"train\": [], \"val\": []}  # 计算训练和验证集上的损失\n",
        "        with torch.no_grad():\n",
        "            for split in ['train', 'val']:\n",
        "                for _ in range(self.config.evaluation_loops):\n",
        "                    input_batch, target_batch = self.data_loader.get_batch(split)\n",
        "                    _, loss = self.model(input_batch, target_batch)\n",
        "                    losses[split].append(loss.item())\n",
        "        self.model.train()  # 切回训练模式\n",
        "\n",
        "        # 计算每个数据集（train，val）的平均损失\n",
        "        return {split: sum(values) / len(values) for split, values in losses.items()}\n",
        "\n",
        "    def train(self):\n",
        "        # 按配置的次数运行train_step\n",
        "        for step in range(self.config.total_training_steps):\n",
        "\n",
        "            # 每100步或最后一步进行一次评估\n",
        "            if step % self.config.evaluation_frequency == 0 or step == self.config.total_training_steps - 1:\n",
        "                eval_loss = self.evaluate()\n",
        "                print(f\"Step {step}: Train Loss {eval_loss['train']:.4f}, Validation Loss {eval_loss['val']:.4f}\")\n",
        "\n",
        "            # 一步训练（每次都要做的核心过程）\n",
        "            train_loss = self.train_step()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPN1h-k_xejG"
      },
      "source": [
        "**Chapter 11: Trainer Class: Section 1: Class Definition** <label><input type=\"checkbox\"> Mark as Done</label>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU53fOfBPA_s"
      },
      "source": [
        "### **Section 2: Class Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NcZr_SN-Ufm"
      },
      "source": [
        "从 Chapter 1 到 Chapter 11，粘贴所有类。<br>\n",
        "**将 DeterministicDropout 替换为 nn.Dropout。**\n",
        "\n",
        "[Watch the video!](https://youtu.be/j2ErzvlslKA)\n",
        "- 无音频\n",
        "- 4分钟\n",
        "\n",
        "视频中，文件名是 answer_colab，但不用在意。\n",
        "\n",
        "```python\n",
        "AttentionHead: dropout = nn.Dropout(config.dropout_rate)\n",
        "MultiHeadAttention: dropout = nn.Dropout(config.dropout_rate)\n",
        "FeedForward:  nn.Dropout(config.dropout_rate)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnLnj31rPTLC"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, text, config):\n",
        "        self.config = config  # 配置对象\n",
        "        chars = sorted(list(set(text)))  # 对唯一字符进行排序\n",
        "        self.ctoi = {char: index for index, char in enumerate(chars)}\n",
        "        self.itoc = {index: char for index, char in enumerate(chars)}\n",
        "        self.vocab_size = len(chars)\n",
        "\n",
        "        # 编码并转换为张量。\n",
        "        # 调用 __init__ 方法外的其他方法或参数时，需要使用 self.\n",
        "        self.data = torch.tensor(self.encode(text), dtype=torch.long)\n",
        "\n",
        "        # 划分为训练/验证数据。\n",
        "        # 即使未指定参数，也默认使用 self.data。\n",
        "        self.train_data, self.val_data = self.split_data()\n",
        "\n",
        "    def encode(self, text):\n",
        "        # 将字符串转换为索引列。调用其他方法或参数时需要使用 self.。\n",
        "        return [self.ctoi[c] for c in text]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return ''.join([self.itoc[i] for i in indices])\n",
        "\n",
        "    def split_data(self):\n",
        "        split_index = int(0.9 * len(self.data))  # 划分点，用于生成90%的训练数据。\n",
        "        return self.data[:split_index], self.data[split_index:]\n",
        "\n",
        "    def get_batch(self, split):\n",
        "        data = self.train_data if split == 'train' else self.val_data\n",
        "        start_indices = torch.randint(len(data) - self.config.input_sequence_length, (self.config.batch_size,)) # 生成提取起始索引\n",
        "\n",
        "        input_sequences = torch.stack([\n",
        "            data[start_index:start_index + self.config.input_sequence_length]\n",
        "            for start_index in start_indices\n",
        "        ])\n",
        "        target_sequences = torch.stack([\n",
        "            data[start_index + 1:start_index + self.config.input_sequence_length + 1]\n",
        "            for start_index in start_indices\n",
        "        ])\n",
        "        return input_sequences.to(self.config.device_type), target_sequences.to(self.config.device_type)\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        # 定义一个嵌入表，大小为词汇量 x 嵌入维度数\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def embed(self, input_indices):\n",
        "        # 获取对应输入索引的嵌入向量\n",
        "        return self.token_embedding_table.forward(input_indices)\n",
        "\n",
        "class PositionEmbedding(nn.Module):\n",
        "    def __init__(self, input_sequence_length = 8, embedding_dim = 8):\n",
        "        super().__init__()\n",
        "        # 位置嵌入层\n",
        "        self.position_embedding_layer = nn.Embedding(input_sequence_length, embedding_dim)\n",
        "\n",
        "    def forward(self, input_indices):\n",
        "        # 输入张量 input_indices 的形状：[批次大小，序列长度]。\n",
        "        sequence_length = input_indices.shape[1]\n",
        "\n",
        "        # 根据序列长度创建位置索引（例如 [0, 1, 2, ..., sequence_length-1]）\n",
        "        position_indices = torch.arange(sequence_length, device=input_indices.device)\n",
        "\n",
        "        # 获取位置索引的嵌入向量\n",
        "        position_embeddings = self.position_embedding_layer.forward(position_indices)\n",
        "\n",
        "        return position_embeddings\n",
        "\n",
        "class EmbeddingModule(nn.Module):\n",
        "    def __init__(self, vocab_size, config):\n",
        "        super().__init__()\n",
        "        # 每个标记的嵌入层\n",
        "        self.token_embedding_layer = TokenEmbedding(vocab_size = vocab_size, embedding_dim = config.embedding_dim)  # 词嵌入层\n",
        "        self.position_embedding_layer = PositionEmbedding(input_sequence_length = config.input_sequence_length, embedding_dim = config.embedding_dim)  # 嵌入位置信息\n",
        "\n",
        "    def forward(self, input_indices):\n",
        "        # 获取token的嵌入表示\n",
        "        token_embeddings = self.token_embedding_layer.embed(input_indices)\n",
        "\n",
        "        # 获取位置嵌入\n",
        "        position_embeddings = self.position_embedding_layer.forward(input_indices)\n",
        "\n",
        "        # 添加词元嵌入和位置嵌入\n",
        "        embeddings = position_embeddings + token_embeddings\n",
        "        return embeddings\n",
        "\n",
        "class LayerNorm(nn.Module):  # 这里继承 nn.Module\n",
        "    def __init__(self, token_length, eps=1e-5, norm_dim=-1):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.norm_dim = norm_dim\n",
        "\n",
        "        # 将 gamma 和 beta 注册为 nn.Parameter，以便在 CPU 和 CUDA 上使用\n",
        "        self.gamma = nn.Parameter(torch.ones(token_length))\n",
        "        self.beta = nn.Parameter(torch.zeros(token_length))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = torch.mean(x, dim=self.norm_dim, keepdim=True)\n",
        "        var = torch.var(x, dim=self.norm_dim, keepdim=True, unbiased=False)\n",
        "        hat = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        output =  self.gamma * hat + self.beta\n",
        "        return output\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, head_size, config):\n",
        "        super().__init__()\n",
        "        self.key_fc= nn.Linear(config.embedding_dim, head_size, bias=False)\n",
        "        self.query_fc = nn.Linear(config.embedding_dim, head_size, bias=False)\n",
        "        self.value_fc = nn.Linear(config.embedding_dim, head_size, bias=False)\n",
        "\n",
        "        # 掩码是通过下三角矩阵创建的（保持自注意力的因果关系）\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.input_sequence_length, config.input_sequence_length)))\n",
        "\n",
        "        # Dropout（确定性版本单独定义）\n",
        "        self.dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "        self.head_size = head_size\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        B, T, C = input_tensor.shape  # 批次，标记长度，嵌入通道\n",
        "\n",
        "        Key = self.key_fc.forward(input_tensor)     # （B，T，head_size）\n",
        "        Query = self.query_fc.forward(input_tensor)   # (B, T, head_size)\n",
        "        Value = self.value_fc.forward(input_tensor)   # (B, T, head_size)\n",
        "\n",
        "        # 计算注意力分数 (QK^T) / sqrt(embedding_dim)\n",
        "        attention_weights_before_mask = Query @ Key.transpose(-2, -1) * self.head_size**(-0.5)\n",
        "\n",
        "        # 应用了蒙版\n",
        "        mask = torch.triu(torch.ones(T, T), diagonal=1).to(input_tensor.device)\n",
        "        masked_attention_weights = attention_weights_before_mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "        # Softmax → Dropout → 加权和\n",
        "        attention_weights = F.softmax(masked_attention_weights, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        out = attention_weights @ Value  # (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.embedding_dim = config.embedding_dim\n",
        "        self.head_size = int(self.embedding_dim / self.num_attention_heads)\n",
        "\n",
        "        # 使用 ModuleList 管理多个头部\n",
        "        self.attention_heads = nn.ModuleList([\n",
        "            AttentionHead(self.head_size, config)\n",
        "            for _ in range(self.num_attention_heads)\n",
        "        ])\n",
        "\n",
        "        # 用于混合每个头输出的线性层\n",
        "        self.output_projection = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
        "\n",
        "        # 输出层的丢弃法\n",
        "        self.dropout = nn.Dropout(config.dropout_rate)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # 获取每个头的输出\n",
        "        # （B，T，head_size）的列表\n",
        "        head_outputs_list = [head.forward(input_tensor) for head in self.attention_heads]\n",
        "\n",
        "        # 将所有头的输出拼接 → (B, T, embedding_dim)\n",
        "        concatenated = torch.cat(head_outputs_list, dim=-1)\n",
        "\n",
        "        # 将输出与线性变换混合\n",
        "        projected = self.output_projection.forward(concatenated)\n",
        "\n",
        "        # 对最终输出应用 dropout\n",
        "        output = self.dropout.forward(projected)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.embedding_dim, config.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.hidden_dim, config.embedding_dim),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        return self.net(input_tensor)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # 每个 LayerNorm 实例都会存储自己的缩放参数 beta 和 gamma。\n",
        "        self.layer_norm1 = nn.LayerNorm(config.embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(config.embedding_dim)\n",
        "\n",
        "        self.multihead_attention = MultiHeadAttention(config=config)\n",
        "        self.feed_forward = FeedForward(config=config)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # forward 方法已省略。\n",
        "        normed_input = self.layer_norm1(input_tensor) # 对输入应用层归一化\n",
        "        attention_output = self.multihead_attention(normed_input) # 应用多头注意力\n",
        "        residual_attention = attention_output + input_tensor # 添加 \"before! layernorm1\"\n",
        "        normed_attention = self.layer_norm2(residual_attention) # 再次对残差输出应用 LayerNorm\n",
        "        feedforward_output = self.feed_forward(normed_attention) # 应用前馈网络 (FFN)\n",
        "        final_output = feedforward_output + residual_attention # 添加 \"before\" layernorm2！\n",
        "\n",
        "        return final_output\n",
        "\n",
        "class VocabularyLogits(nn.Module):\n",
        "    def __init__(self, vocab_size, config):\n",
        "        super().__init__()\n",
        "        # 层归一化\n",
        "        self.output_norm = nn.LayerNorm(config.embedding_dim)\n",
        "        # 词汇量投影\n",
        "        self.vocab_projection = nn.Linear(config.embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, transformer_block_output):\n",
        "        # 对Transformer块的输出应用层归一化。\n",
        "        normalized_output = self.output_norm.forward(transformer_block_output)  # （批次大小，时间步长，通道数）\n",
        "\n",
        "        # 使用线性层将输入分数投影到词汇表大小的维度。\n",
        "        vocab_logits = self.vocab_projection.forward(normalized_output)  # （批次大小，时间步长，词汇表大小）\n",
        "\n",
        "        return vocab_logits\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, config):\n",
        "        super().__init__()\n",
        "        self.config = config  # 生成时也会用到，所以保留它。\n",
        "        self.embedding = EmbeddingModule(vocab_size, config=config)\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(config=config) for _ in range(config.layer_count)])\n",
        "        self.vocab_projection = VocabularyLogits(vocab_size=vocab_size, config=config)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 生成文本\n",
        "    def generate(self, input_indices, max_new_tokens):\n",
        "        # 仅生成指定数量的标记 max_new_tokens\n",
        "        for _ in range(max_new_tokens):\n",
        "            input_conditioned = input_indices[:, -self.config.input_sequence_length:] # 剪辑输入\n",
        "\n",
        "            # 前向传播返回 `(likelihood, loss)`——只存储 `likelihood` 作为 `logits`。\n",
        "            logits, _ = self.forward(input_conditioned, target_indices=None)\n",
        "            last_logits = logits[:, -1, :] # 提取最后一个标记位置的logit值\n",
        "            probs = F.softmax(last_logits, dim=-1) # 使用Softmax将似然转换为概率\n",
        "\n",
        "            # 采样下一个标记\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # 合并新令牌，更新 `input_indices`。\n",
        "            input_indices = torch.cat((input_indices, next_token), dim=1)\n",
        "\n",
        "        # 返回最终的 input_indices。长度为原始 input_indices 加上 max_new_tokens\n",
        "        return input_indices\n",
        "\n",
        "    # 计算似然和损失\n",
        "    def forward(self, input_indices, target_indices):\n",
        "        embeddings = self.embedding(input_indices)\n",
        "        blocks_output = self.blocks(embeddings)\n",
        "        logits = self.vocab_projection(blocks_output)\n",
        "\n",
        "        # 推理阶段没有目标，因此损失为 None\n",
        "        # —只返回似然性（logits）。\n",
        "        if target_indices is None:\n",
        "            return logits, None\n",
        "\n",
        "        batch_size, token_len, vocab_size = logits.shape\n",
        "        logits = logits.view(batch_size * token_len, vocab_size)\n",
        "        targets = target_indices.view(batch_size * token_len)\n",
        "        loss = self.criterion(logits, targets)\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0vcUT2kwwR5"
      },
      "source": [
        "**用 nn.Dropout 替换 DeterministicDropout**\n",
        "```python\n",
        "AttentionHead: dropout = nn.Dropout(config.dropout_rate)\n",
        "MultiHeadAttention: dropout = nn.Dropout(config.dropout_rate)\n",
        "FeedForward:  nn.Dropout(config.dropout_rate)\n",
        "```\n",
        "<label><input type=\"checkbox\"> Done</label>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAqEd0cywV56"
      },
      "source": [
        "**Chapter 12: Trainer Class: Section 2: Class Summary** <label><input type=\"checkbox\"> 标记为已完成</label>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IGG8rZJT9fx"
      },
      "source": [
        "### **Section 3: Training and Inference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pdT24q93vOX"
      },
      "source": [
        "到目前为止，embedding 维度是8，有2个注意力头，FeedForward 网络的隐藏层维度是16。<br>\n",
        "这样表达能力太有限了。<br>\n",
        "现在，将embedding维度设置为64，注意力头设为4，FeedForward 网络的隐藏层维度改为256。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ooeKTSmYOdn",
        "outputId": "505d4f44-f337-48eb-dde3-e5e4198a9b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Each setting for the ModelConfig class:\n",
            "Batch size: 16\n",
            "Input sequence length: 32\n",
            "Total training steps: 5000\n",
            "Evaluation frequency (in steps): 100\n",
            "Learning rate: 0.001\n",
            "Device in use: cuda\n",
            "Number of evaluation loops: 10\n",
            "Embedding vector dimension: 64\n",
            "Hidden layer dimension of the feedforward network: 256\n",
            "Number of attention heads: 4\n",
            "Number of model layers: 4\n",
            "Dropout rate: 0.1\n",
            "Random seed value: 1337\n"
          ]
        }
      ],
      "source": [
        "# 存储模型设置的配置类\n",
        "class ModelConfig:\n",
        "    batch_size = 16  # 一次处理的数据数量（批量大小）\n",
        "    input_sequence_length = 32  # 输入数据长度（序列长度）\n",
        "    total_training_steps = 5000  # 最大训练次数（步数）\n",
        "    evaluation_frequency = 100  # 模型性能评估频率\n",
        "    learning_rate = 0.001  # 学习率\n",
        "    device_type = 'cuda' if torch.cuda.is_available() else 'cpu'  # 使用的设备（GPU 或 CPU）\n",
        "    evaluation_loops = 10  # 评估期间的重复次数\n",
        "    embedding_dim = 64  # 嵌入层大小（特征向量的维度数）\n",
        "    hidden_dim = 256\n",
        "    num_attention_heads = 4  # 注意机制头数\n",
        "    layer_count = 4  # 模型中的层数\n",
        "    dropout_rate = 0.1  # 丢弃概率\n",
        "    random_seed_value = 1337  # 用于随机数种子以保证可复现性\n",
        "\n",
        "# 验证您的设置\n",
        "config = ModelConfig()\n",
        "\n",
        "print(\"ModelConfig 类的每个设置：\")\n",
        "print(f\"Batch size: {config.batch_size}\")\n",
        "print(f\"Input sequence length: {config.input_sequence_length}\")\n",
        "print(f\"Total training steps: {config.total_training_steps}\")\n",
        "print(f\"Evaluation frequency (in steps): {config.evaluation_frequency}\")\n",
        "print(f\"Learning rate: {config.learning_rate}\")\n",
        "print(f\"Device in use: {config.device_type}\")\n",
        "print(f\"Number of evaluation loops: {config.evaluation_loops}\")\n",
        "print(f\"Embedding vector dimension: {config.embedding_dim}\")\n",
        "print(f\"Hidden layer dimension of the feedforward network: {config.hidden_dim}\")\n",
        "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
        "print(f\"Number of model layers: {config.layer_count}\")\n",
        "print(f\"Dropout rate: {config.dropout_rate}\")\n",
        "print(f\"Random seed value: {config.random_seed_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwvOpyQPm5Jl"
      },
      "source": [
        "**`Check Point`**\n",
        "<label><input type=\"checkbox\"> 确认 Config 类中的每个设置都显示正确<br></label>\n",
        "- 批量大小: 16<br>\n",
        "- 块大小: 32<br>\n",
        "- 最大迭代次数: 5000<br>\n",
        "- 评估间隔: 100<br>\n",
        "- 学习率: 0.001<br>\n",
        "- 使用设备: cuda 或 cpu<br>\n",
        "- 评估迭代次数: 10<br>\n",
        "- 嵌入层维度: 64<br>\n",
        "- 前馈隐藏层维度: 256<br>\n",
        "- 注意力头数: 4<br>\n",
        "- 模型层数: 4<br>\n",
        "- Dropout 比例: 0.1<br>\n",
        "- 随机种子: 1337<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EctA8L49Tsc0",
        "outputId": "d7dfe830-e4bb-4f40-8b15-8cda7e56de12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x782b68401490>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 加载配置并设置随机种子\n",
        "config = ModelConfig()\n",
        "torch.manual_seed(config.random_seed_value)  # 设置随机数种子以确保结果可复现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXBa9z1ogsUw"
      },
      "outputs": [],
      "source": [
        "# 加载数据\n",
        "with open(\"input.txt\", 'r', encoding = 'utf-8') as f:\n",
        "    text_data = f.read()\n",
        "data_loader = DataLoader(text_data, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtyVxmwiTotc",
        "outputId": "c124075e-6a3d-413a-f0f9-ef56415b2dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.209729 M parameters\n"
          ]
        }
      ],
      "source": [
        "# 初始化模型和优化器\n",
        "model = BigramLanguageModel(vocab_size = data_loader.vocab_size, config = config).to(config.device_type)  # 指定您使用的设备\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "# 打印模型中的参数数量\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mysHV8yjE7P6"
      },
      "source": [
        "相比之下，GPT2-Small 模型有 1.17 亿（或 1.24 亿）参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx-g4ndFubat"
      },
      "source": [
        "作为测试，试着在预训练阶段进行生成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpnodhC5ubat",
        "outputId": "77b04870-2352-4921-fbea-3ad7f62188ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's henvIeuW;JcijkeLWfUaUJW;VcE!Pf;ocFF&oNhq$eQLJOOEFWXgjNhhqv;Es\n",
            "iowD&ZqK&CgwN'Pq$mFHGjX.esumfonqUzgrN?pNVvN!Iebiqfs!EuOt3Zw?Bjx$oYk-wXmvevRibdVde!eJgRLKasNnge?DEYpK! 'scfoOl!Ebe$iol$-UpfXGKtewgLMsO!?fX?&D?;-$zBR.SudGdOo.&co\n",
            "zvzNqQriRR'QbHbs'QqXghiHWJwLUEZE&pNz\n",
            "T'Rk!ZgbN?tmE.uJaekBK?Oh&n&Um,LDqc'omcC&Z;xpZGipgRQeN$y?VDbOvsN,$IcNhepTHJeWkzKdrf?roHm?dfwFUpwMVg;ei&$RCXTyowaFZhjVBm$3g33cAuh,K?UlAGcX;p!JUlNvvbIHG.3inUc.HjMCsyhnpwAKylbSHT'pXh3UNfO:mreo'VrL'cpe-NC,ntZAziOpKcpTOE.hs:Ck&z'LGJgyb3?p!3fI,OjzFHE\n"
          ]
        }
      ],
      "source": [
        "text = \"Let's he\"  # 提示\n",
        "initial_context = torch.tensor(data_loader.encode(text), dtype=torch.long)\n",
        "# 使用 unsqueeze(0) 添加批次维度（批量大小=1）\n",
        "initial_context_unsqueeze = initial_context.unsqueeze(0)\n",
        "# 关键是要将其移动到你正在使用的设备（CPU或GPU）上！\n",
        "initial_context_unsqueeze = initial_context_unsqueeze.to(config.device_type)\n",
        "\n",
        "# 开始生成\n",
        "generated_sequence_initial = model.generate(initial_context_unsqueeze, max_new_tokens=500)\n",
        "print(data_loader.decode(generated_sequence_initial[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYintz_Aubat"
      },
      "source": [
        "嗯，这是台电脑💻，是吧？\n",
        "\n",
        "那好，咱们训练它，把它养成宝宝👶！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK_3F336Eu6z"
      },
      "source": [
        "**终于到了最后一部分。开启[情感背景音乐¹](https://youtu.be/GqmAe0QfkjU?feature=shared)。<br>训练大约需要2到4分钟。让我们静静感受这一刻。**\n",
        "\n",
        "---\n",
        "\n",
        "内容参考：  \n",
        "¹ **DooPiano**，《BTS (방탄소년단) – 봄날 (Spring Day) 钢琴与弦乐版本》，YouTube，3:41，发布于约8.2年前（大约2017年）。访问时间：2025年7月8日。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6t0rayVFh2U"
      },
      "source": [
        "**请务必听一下背景音乐！保证让你感动！现在，运行下面的单元开始训练吧！**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpagygD5TqHA",
        "outputId": "87c4c733-8f0d-45da-fe25-cd8b2fd6d5a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===TRAINING STARTED SUCCESSFULLY===\n",
            "Step 0: Train Loss 2.3080, Validation Loss 2.3095\n",
            "Step 100: Train Loss 2.2592, Validation Loss 2.2887\n",
            "Step 200: Train Loss 2.2122, Validation Loss 2.2228\n",
            "Step 300: Train Loss 2.1850, Validation Loss 2.1929\n",
            "Step 400: Train Loss 2.1433, Validation Loss 2.1610\n",
            "Step 500: Train Loss 2.0731, Validation Loss 2.1434\n",
            "Step 600: Train Loss 2.0713, Validation Loss 2.1265\n",
            "Step 700: Train Loss 2.0781, Validation Loss 2.0784\n",
            "Step 800: Train Loss 2.0399, Validation Loss 2.1171\n",
            "Step 900: Train Loss 1.9939, Validation Loss 2.0921\n",
            "Step 1000: Train Loss 2.0463, Validation Loss 2.0800\n",
            "Step 1100: Train Loss 2.0031, Validation Loss 2.0437\n",
            "Step 1200: Train Loss 1.9710, Validation Loss 2.0322\n",
            "Step 1300: Train Loss 1.8837, Validation Loss 2.0148\n",
            "Step 1400: Train Loss 1.9525, Validation Loss 2.0299\n",
            "Step 1500: Train Loss 1.9432, Validation Loss 2.0055\n",
            "Step 1600: Train Loss 1.9284, Validation Loss 1.9633\n",
            "Step 1700: Train Loss 1.9180, Validation Loss 1.9909\n",
            "Step 1800: Train Loss 1.8999, Validation Loss 1.9350\n",
            "Step 1900: Train Loss 1.8392, Validation Loss 1.9623\n",
            "Step 2000: Train Loss 1.8845, Validation Loss 1.9732\n",
            "Step 2100: Train Loss 1.8869, Validation Loss 1.9537\n",
            "Step 2200: Train Loss 1.8619, Validation Loss 1.9648\n",
            "Step 2300: Train Loss 1.8186, Validation Loss 1.9966\n",
            "Step 2400: Train Loss 1.8383, Validation Loss 1.9590\n",
            "Step 2500: Train Loss 1.8500, Validation Loss 1.9092\n",
            "Step 2600: Train Loss 1.8149, Validation Loss 1.8838\n",
            "Step 2700: Train Loss 1.8128, Validation Loss 1.9213\n",
            "Step 2800: Train Loss 1.8046, Validation Loss 1.9234\n",
            "Step 2900: Train Loss 1.8057, Validation Loss 1.9302\n",
            "Step 3000: Train Loss 1.7888, Validation Loss 1.8733\n",
            "Step 3100: Train Loss 1.7705, Validation Loss 1.9013\n",
            "Step 3200: Train Loss 1.7730, Validation Loss 1.9126\n",
            "Step 3300: Train Loss 1.8045, Validation Loss 1.9461\n",
            "Step 3400: Train Loss 1.7554, Validation Loss 1.9231\n",
            "Step 3500: Train Loss 1.7937, Validation Loss 1.8936\n",
            "Step 3600: Train Loss 1.7712, Validation Loss 1.8767\n",
            "Step 3700: Train Loss 1.7668, Validation Loss 1.8839\n",
            "Step 3800: Train Loss 1.7718, Validation Loss 1.8480\n",
            "Step 3900: Train Loss 1.7603, Validation Loss 1.8709\n",
            "Step 4000: Train Loss 1.7487, Validation Loss 1.8557\n",
            "Step 4100: Train Loss 1.7145, Validation Loss 1.8529\n",
            "Step 4200: Train Loss 1.7628, Validation Loss 1.8611\n",
            "Step 4300: Train Loss 1.6964, Validation Loss 1.8522\n",
            "Step 4400: Train Loss 1.7155, Validation Loss 1.8929\n",
            "Step 4500: Train Loss 1.7233, Validation Loss 1.8741\n",
            "Step 4600: Train Loss 1.7043, Validation Loss 1.8837\n",
            "Step 4700: Train Loss 1.6532, Validation Loss 1.8430\n",
            "Step 4800: Train Loss 1.6850, Validation Loss 1.7992\n",
            "Step 4900: Train Loss 1.7202, Validation Loss 1.8850\n",
            "Step 4999: Train Loss 1.7031, Validation Loss 1.9120\n",
            "Training DONE\n",
            "Model and optimizer state saved to bigram_language_model.pth\n"
          ]
        }
      ],
      "source": [
        "print(\"===训练已成功开始===\")\n",
        "\n",
        "# 训练模型\n",
        "trainer = Trainer(model, optimizer, data_loader, config)\n",
        "trainer.train()\n",
        "\n",
        "# 保存模型\n",
        "save_path = \"bigram_language_model.pth\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}, save_path)\n",
        "\n",
        "print(\"训练完成\")\n",
        "print(f\"Model and optimizer state saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiEiQNKci0xq",
        "outputId": "76f971cc-7a84-4c47-c9a6-dac8152999fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Model Loaded Successfully!=====\n"
          ]
        }
      ],
      "source": [
        "# ---- 加载已保存模型并生成文本 -----\n",
        "# 初始化新模型和优化器（需相同设置和类定义）\n",
        "loaded_model = BigramLanguageModel(vocab_size = data_loader.vocab_size, config = config).to(config.device_type)  # 指定您使用的设备\n",
        "loaded_optimizer = torch.optim.AdamW(loaded_model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "save_path = \"bigram_language_model.pth\"\n",
        "checkpoint = torch.load(save_path, map_location=config.device_type)\n",
        "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "loaded_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "print(\"=====模型加载成功！=====\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Pj_bPCEPPS"
      },
      "source": [
        "将模型设置为评估模式。生成前请关闭 Dropout。\n",
        "\n",
        "如果忘了，Dropout 会开启，输出会很糟糕，请务必注意。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yuLBig8EG91",
        "outputId": "83214a4c-14a2-4649-d21a-e1e6755d5a73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Set to Evaluation mode, disabled Dropout. =====\n"
          ]
        }
      ],
      "source": [
        "loaded_model.eval()\n",
        "print(\"===== 设置为评估模式，已禁用 Dropout。 =====\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaQb254DuA8X",
        "outputId": "b6f93c3a-e5ce-47b6-b2d3-d61613389e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoded_prompt\n",
            "Tensor Size: [8]\n",
            "tensor([\n",
            "         24.00,  43.00,  58.00,   5.00,  57.00,   1.00,  46.00,  43.00\n",
            "       ])\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Let's he\"  # 提示\n",
        "encoded_prompt = torch.tensor(data_loader.encode(prompt), dtype=torch.long)\n",
        "print_formatted_tensor(\"编码提示\", encoded_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QoDkdBPuC2Q",
        "outputId": "cf25d96f-7b3b-4fc3-bc7a-80252e47a5d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoded_prompt_unsqueeze\n",
            "Tensor Size: [1, 8]\n",
            "tensor([\n",
            "         [ 24.00,  43.00,  58.00,   5.00,  57.00,   1.00,  46.00,  43.00]\n",
            "       ])\n"
          ]
        }
      ],
      "source": [
        "# 使用 unsqueeze(0) 添加批次维度（批次大小=1）\n",
        "encoded_prompt_unsqueeze = encoded_prompt.unsqueeze(0)\n",
        "print_formatted_tensor(\"编码提示扩展维度\", encoded_prompt_unsqueeze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dUOQgT3uEHx"
      },
      "outputs": [],
      "source": [
        "# ↓ 关键是要将提示张量移动到你正在使用的设备（CPU或GPU）！\n",
        "encoded_prompt_unsqueeze = encoded_prompt_unsqueeze.to(config.device_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdiKzDt6zXpP"
      },
      "source": [
        "```python\n",
        "Instance: loaded_model\n",
        "Method: generate\n",
        "Arguments: encoded_prompt_unsqueeze, max_new_tokens=1000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U58DsEfuF27",
        "outputId": "41bb755e-a8bc-4109-b385-79ede0010b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's he weath to begiver thing thou are perpisency the gertater sing\n",
            "To With beenes; no from to the chaul!\n",
            "\n",
            "JUTIO:\n",
            "Were this not the carence your in singrard hath lond\n",
            "Capus leet.\n",
            "\n",
            "BRUTUS:\n",
            "Thee bust to me to speaks or his it,\n",
            "\n",
            "Deseetock untertay laid I wand for shing\n",
            "that ittience, God nig for the from do it it.\n",
            "\n",
            "Pervore, I sethall thou breather,\n",
            "By tyre agent man some thou my servery plaid be the spoble.\n",
            "\n",
            "GLOUCESTER:\n",
            "Shallo, Say fauls I weick.\n",
            "\n",
            "DUCESTER MARGAREY:\n",
            "My noble thear, an, and at to he's guend\n",
            "For me full I say's be swould Gentleat haph till\n",
            "Nor And being years their rlancues\n",
            "The Last burssened to her be gry our sto stoot\n",
            "Ands that nurse steep\n",
            "Of aund will of her age man, tiSingment your boy's rings:\n",
            "Your my and again and well I would theur ady\n",
            "A samernon to do time'n sail thou lack your juty,\n",
            "No? I coundertife thou have thee other awarthrough a dne'd I preaty\n",
            "Waveichard him time if our good bowers highne:\n",
            "Thears.\n",
            "\n",
            "Thidds; and I' weal theseity. Bollow Richions:\n",
            "As City you?I heark,\n"
          ]
        }
      ],
      "source": [
        "# 开始生成\n",
        "generated_sequence = loaded_model.generate(encoded_prompt_unsqueeze, max_new_tokens=1000) # TODO: 实例，\n",
        "print(data_loader.decode(generated_sequence[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcHLCQ6P4O-8"
      },
      "source": [
        "它进步到婴儿👶水平了吗？\n",
        "\n",
        "这次用nanoGPT，它从电脑💻升级成了婴儿👶。\n",
        "\n",
        "下一步，GPT2将从婴儿👶迈向初中生🧑！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R52XY3hdxKUS"
      },
      "source": [
        "**Chapter 12: Trainer Class: Section 3: 训练与推理** <label><input type=\"checkbox\"> Mark as Done</label>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viJYZTXwxNhC"
      },
      "source": [
        "**Chapter 12: The Trainer Class** <label><input type=\"checkbox\"> Mark as Done</label>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1dLHoKgxQwm"
      },
      "source": [
        "**`nanoGPT`** <label><input type=\"checkbox\"> 完成</label>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykz9QCl8ubau"
      },
      "source": [
        "![スクリーンショット 2025-06-25 0.58.23.png](data:image/png;base64,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)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
