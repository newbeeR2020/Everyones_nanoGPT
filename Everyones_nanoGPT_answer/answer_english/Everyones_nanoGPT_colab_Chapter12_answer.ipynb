{"cells":[{"cell_type":"markdown","source":["# **NOTE for Colab Users**\n","\n","# **Do NOT write directly in this file‚Äîyour work might be lost!**\n","\n","# **Always make a copy before you start.**\n","\n","How to make a copy\n","\n","1. Click \"File\" in the top left.\n","> *If you can‚Äôt find the header (like ‚ÄúFile‚Äù or \"Runtime\"), click the ‚Äúv‚Äù mark at the top right to show it.*\n","\n","2. \"Save a copy in Drive\"\n","\n","3. Change the copied file‚Äôs name to ‚ÄúYOURNAMEs\\_FileName.ipynb‚Äù.\n","> Ex: If your name is Olivia : Olivias_FileName.ipynb\n","\n","\n","\n","---\n","\n","* Check marks (‚úÖ) won‚Äôt be saved. If you reload the page with Chrome‚Äôs refresh button, they‚Äôll disappear.<br>\n","If you want to pause and come back later, just add a text cell and write something like ‚ÄúSO FAR DONE.‚Äù\n","\n","---\n","\n","* In Colab, **previous outputs reset every 30 to 90 minutes**.<br>\n","  Because of this, errors like `~~ is not defined` will happen **very often**.\n","\n","  üîÅ What to do when you get an `~~is not defined` error\n","  1. First, make sure your variable is spelled right.<br>\n","  2. If it‚Äôs spelled right but you still see the error, **click the cell you want to restart**.<br>\n","  3. Go to ‚ÄúRuntime‚Äù (top left) ‚Üí Click ‚ÄúRun before‚Äù.<br>\n","    ‚Üí This will **re-run all previous cells**.\n","  4. Run the cell again.\n","\n","  If you still see the error after these steps,<br>\n","  there might be a basic mistake in your TODO answers from earlier cells.<br>\n","  Please check if your answer is right.<br>\n","  Or ask ChatGPT or another coding assistant for help."],"metadata":{"id":"Cj6PNj59iqJ8"}},{"cell_type":"markdown","source":["# **Preparation**\n","\n","This section only loads content from earlier chapters.<br>\n","Just run the code‚Äîyou don‚Äôt need to read it.<br>\n","Feel free to skip ahead.<br>"],"metadata":{"id":"BXZzMLzXiseX"}},{"cell_type":"code","source":["# Donwload the file\n","!wget https://raw.githubusercontent.com/HayatoHongo/Everyones_nanoGPT/main/input.txt -O input.txt\n","# Load the file called input.text that you just downloaded in utf-8.\n","with open(\"input.txt\", 'r', encoding = 'utf-8') as f:\n","    text = f.read()\n","\n","# A Function to Display Tensors Nicely (Feel Free to Skip This)\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","def print_formatted_tensor(*args, width=6, decimals=2):\n","    \"\"\"\n","    A function that neatly formats and displays a PyTorch Tensor, and also prints its size.\n","\n","    Example usage:\n","        print_formatted_tensor(\"name\", tensor)\n","        print_formatted_tensor(tensor)\n","\n","    Args:\n","        *args: If given 1 argument, it is treated as a tensor.\n","               If given 2 arguments, the first is treated as the name, the second as the tensor.\n","        width (int): Display width for each number (default: 6)\n","        decimals (int): Number of decimal places to show (default: 2)\n","    \"\"\"\n","\n","    # Determine tensor and name from arguments\n","    if not args:\n","        raise ValueError(\"At least one argument is required.\")\n","    if isinstance(args[0], str):\n","        if len(args) < 2:\n","            raise ValueError(\"Tensor is not specified.\")\n","        name, tensor = args[0], args[1]\n","    else:\n","        name, tensor = None, args[0]\n","\n","    # Convert Tensor to List\n","    tensor_list = tensor.detach().cpu().tolist()\n","\n","    def format_list(lst, indent):\n","        \"\"\"Formatting a recursively nested list and returning a string\"\"\"\n","        # If the contents are lists, then re-return\n","        if isinstance(lst, list) and lst and isinstance(lst[0], list):\n","            inner = \",\\n\".join(\" \" * indent + format_list(sub, indent + 2) for sub in lst)\n","            return \"[\\n\" + inner + \"\\n\" + \" \" * (indent - 2) + \"]\"\n","        # For numerical lists\n","        return \"[\" + \", \".join(f\"{v:{width}.{decimals}f}\" for v in lst) + \"]\"\n","\n","    # Formatted string (bar brackets on outermost frames are removed)\n","    formatted = format_list(tensor_list, indent=9)\n","    inner_formatted = formatted[1:-1].strip()\n","\n","    # Result output\n","    if name:\n","        print(name)\n","    print(f\"Tensor Size: {list(tensor.size())}\")\n","    print(\"tensor([\")\n","    print(\" \" * 9 + inner_formatted)\n","    print(\" \" * 7 + \"])\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8s04g6yFiuHE","outputId":"a62bb238-1185-4194-d97f-8cbdcdd1f733"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-07-08 11:25:56--  https://raw.githubusercontent.com/HayatoHongo/nanoGPT_todo/main/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‚Äòinput.txt‚Äô\n","\n","\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n","\n","2025-07-08 11:25:56 (31.2 MB/s) - ‚Äòinput.txt‚Äô saved [1115394/1115394]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"xh_2Ls8tn2rk"},"source":["# **Chapter 12 Trainer Class**"]},{"cell_type":"markdown","metadata":{"id":"tEl9A_wk33Pa"},"source":["### **Section 1: Class Definition**"]},{"cell_type":"markdown","metadata":{"id":"lnILkem04RSW"},"source":["üîò **Options**: There may be extra options you don't need.\n","\n","`self.model`„ÄÄ`self.optimizer`„ÄÄ`self.data_loader`„ÄÄ`self.config`„ÄÄ`split_data`„ÄÄ`get_batch`„ÄÄ`'train'`, `'val'`„ÄÄ`input_batch`„ÄÄ`target_batch`„ÄÄ`logits`„ÄÄ`self.config.total_training_steps`„ÄÄ`self.config.evaluation_loops`  \n","`loss`„ÄÄ`backward()`„ÄÄ`self.train_step()`„ÄÄ`self.evaluate()`"]},{"cell_type":"code","source":["class Trainer:\n","    def __init__(self, model, optimizer, data_loader, config):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.data_loader = data_loader\n","        self.config = config\n","\n","    def train_step(self):\n","        # Get a batch for training.\n","        input_batch, target_batch = self.data_loader.get_batch('train')\n","        self.optimizer.zero_grad()\n","\n","        # Model forward pass and loss calculation\n","        logits, loss = self.model(input_batch, target_batch)\n","        loss.backward()  # Backpropagation (Error backpropagation)\n","        self.optimizer.step()  # Update parameters\n","\n","        return loss.item() # Returns the value of the loss\n","\n","    def evaluate(self):\n","        self.model.eval()  # Set to evaluation mode\n","        losses = {\"train\": [], \"val\": []} # Calculate losses on both training and validation data\n","        with torch.no_grad():\n","            for split in ['train', 'val']:\n","                for _ in range(self.config.evaluation_loops):\n","                    input_batch, target_batch = self.data_loader.get_batch(split)\n","                    _, loss = self.model(input_batch, target_batch)\n","                    losses[split].append(loss.item())\n","        self.model.train()  # Return to training mode\n","\n","        # Calculate the average losses for each split (train, val)\n","        return {split: sum(values) / len(values) for split, values in losses.items()}\n","\n","    def train(self):\n","        # Run train_step the number of times specified in config.\n","        for step in range(self.config.total_training_steps):\n","\n","            # Evaluate every 100 iterations or just at the final step.\n","            if step % self.config.evaluation_frequency == 0 or step == self.config.total_training_steps - 1:\n","                eval_loss = self.evaluate()\n","                print(f\"Step {step}: Train Loss {eval_loss['train']:.4f}, Validation Loss {eval_loss['val']:.4f}\")\n","\n","            # One step of training (the main process that you do every time)\n","            train_loss = self.train_step()"],"metadata":{"id":"sRi90bRasayV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mi8-y1ak4VOS","outputId":"ec0f1c70-6f55-4cf2-fb9f-e5b56297691b","colab":{"base_uri":"https://localhost:8080/","height":191}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nclass Trainer:\\n    def __init__(self, model, optimizer, data_loader, config):\\n        self.model = model\\n        self.optimizer = optimizer\\n        self.data_loader = data_loader\\n        self.config = config\\n\\n    def train_step(self):\\n        # Get a batch for training.\\n        input_batch, target_batch = ___________._______(_____)\\n        self.optimizer.zero_grad()\\n\\n        # Model forward pass and loss calculation\\n        logits, loss = _______(_________, __________)\\n        _____.__________  # Backpropagation (Error backpropagation)\\n        self.optimizer.step()  # Update parameters\\n\\n        return loss.item() # Returns the value of the loss\\n\\n    def evaluate(self):\\n        self.model.eval()  # Set to evaluation mode\\n        losses = {\"train\": [], \"val\": []} # Calculate losses on both training and validation data\\n        with torch.no_grad():\\n            for split in [\\'train\\', \\'val\\']:\\n                for _ in range(self.config.evaluation_loops):\\n                    input_batch, target_batch = self.data_loader.get_batch(split)\\n                    _, loss = self.model(input_batch, target_batch)\\n                    losses[split].append(loss.item())\\n        self.model.train()  # Return to training mode\\n\\n        # Calculate the average losses for each split (train, val)\\n        return {split: sum(values) / len(values) for split, values in losses.items()}\\n\\n    def train(self):\\n        # Run train_step the number of times specified in config.\\n        for step in range(_________________________):\\n\\n            # Evaluate every 100 iterations or just at the final step.\\n            if step % self.config.evaluation_frequency == 0 or step == self.config.total_training_steps - 1:\\n                eval_loss = self.evaluate()\\n                print(f\"Step {step}: Train Loss {eval_loss[\\'train\\']:.4f}, Validation Loss {eval_loss[\\'val\\']:.4f}\")\\n\\n            # One step of training (the main process that you do every time)\\n            train_loss = _____________\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["\"\"\"\n","class Trainer:\n","    def __init__(self, model, optimizer, data_loader, config):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.data_loader = data_loader\n","        self.config = config\n","\n","    def train_step(self):\n","        # Get a batch for training.\n","        input_batch, target_batch = ___________._______(_____)\n","        self.optimizer.zero_grad()\n","\n","        # Model forward pass and loss calculation\n","        logits, loss = _______(_________, __________)\n","        _____.__________  # Backpropagation (Error backpropagation)\n","        self.optimizer.step()  # Update parameters\n","\n","        return loss.item() # Returns the value of the loss\n","\n","    def evaluate(self):\n","        self.model.eval()  # Set to evaluation mode\n","        losses = {\"train\": [], \"val\": []} # Calculate losses on both training and validation data\n","        with torch.no_grad():\n","            for split in ['train', 'val']:\n","                for _ in range(self.config.evaluation_loops):\n","                    input_batch, target_batch = self.data_loader.get_batch(split)\n","                    _, loss = self.model(input_batch, target_batch)\n","                    losses[split].append(loss.item())\n","        self.model.train()  # Return to training mode\n","\n","        # Calculate the average losses for each split (train, val)\n","        return {split: sum(values) / len(values) for split, values in losses.items()}\n","\n","    def train(self):\n","        # Run train_step the number of times specified in config.\n","        for step in range(_________________________):\n","\n","            # Evaluate every 100 iterations or just at the final step.\n","            if step % self.config.evaluation_frequency == 0 or step == self.config.total_training_steps - 1:\n","                eval_loss = self.evaluate()\n","                print(f\"Step {step}: Train Loss {eval_loss['train']:.4f}, Validation Loss {eval_loss['val']:.4f}\")\n","\n","            # One step of training (the main process that you do every time)\n","            train_loss = _____________\n","\"\"\"\n"]},{"cell_type":"markdown","source":["<details>\n","<summary>Click here to show/hide the answer</summary>\n","\n","```python\n","class Trainer:\n","    def __init__(self, model, optimizer, data_loader, config):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.data_loader = data_loader\n","        self.config = config\n","\n","    def train_step(self):\n","        # Get a batch for training.\n","        input_batch, target_batch = self.data_loader.get_batch('train')\n","        self.optimizer.zero_grad()\n","\n","        # Model forward pass and loss calculation\n","        logits, loss = self.model(input_batch, target_batch)\n","        loss.backward()  # Backpropagation (Error backpropagation)\n","        self.optimizer.step()  # Update parameters\n","\n","        return loss.item() # Returns the value of the loss\n","\n","    def evaluate(self):\n","        self.model.eval()  # Set to evaluation mode\n","        losses = {\"train\": [], \"val\": []} # Calculate losses on both training and validation data\n","        with torch.no_grad():\n","            for split in ['train', 'val']:\n","                for _ in range(self.config.evaluation_loops):\n","                    input_batch, target_batch = self.data_loader.get_batch(split)\n","                    _, loss = self.model(input_batch, target_batch)\n","                    losses[split].append(loss.item())\n","        self.model.train()  # Return to training mode\n","\n","        # Calculate the average losses for each split (train, val)\n","        return {split: sum(values) / len(values) for split, values in losses.items()}\n","\n","    def train(self):\n","        # Run train_step the number of times specified in config.\n","        for step in range(self.config.total_training_steps):\n","\n","            # Evaluate every 100 iterations or just at the final step.\n","            if step % self.config.evaluation_frequency == 0 or step == self.config.total_training_steps - 1:\n","                eval_loss = self.evaluate()\n","                print(f\"Step {step}: Train Loss {eval_loss['train']:.4f}, Validation Loss {eval_loss['val']:.4f}\")\n","\n","            # One step of training (the main process that you do every time)\n","            train_loss = self.train_step()\n","```"],"metadata":{"id":"7KUW7ceEA4ek"}},{"cell_type":"markdown","metadata":{"id":"PPN1h-k_xejG"},"source":["**`Chapter 11: Trainer Class: Section 1: Class Definition`** <label><input type=\"checkbox\"> Mark as Done</label>"]},{"cell_type":"markdown","metadata":{"id":"DU53fOfBPA_s"},"source":["### **Section 2: Class Summary**"]},{"cell_type":"markdown","metadata":{"id":"8NcZr_SN-Ufm"},"source":["Paste all classes from Chapter 1 to Chapter 11.<br>\n","**Replace DeterministicDropout with nn.Dropout.**\n","\n","[Watch the video!](https://youtu.be/j2ErzvlslKA)\n","- no audio\n","- 4 minutes\n","\n","In the video, the file name is answer_colab, but do not mind.\n","\n","```python\n","AttentionHead: dropout = nn.Dropout(config.dropout_rate)\n","MultiHeadAttention: dropout = nn.Dropout(config.dropout_rate)\n","FeedForward:  nn.Dropout(config.dropout_rate)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnLnj31rPTLC"},"outputs":[],"source":["class DataLoader:\n","    def __init__(self, text, config):\n","        self.config = config  # Configuration Object\n","        chars = sorted(list(set(text)))  # Sorting unique characters\n","        self.ctoi = {char: index for index, char in enumerate(chars)}\n","        self.itoc = {index: char for index, char in enumerate(chars)}\n","        self.vocab_size = len(chars)\n","\n","        # Encode and convert to tensor.\n","        # `self.` is required to call other methods or arguments outside this `__init__` method.\n","        self.data = torch.tensor(self.encode(text), dtype=torch.long)\n","\n","        # Split into training/verification data.\n","        # `self.data` is used by default even if no argument is specified.\n","        self.train_data, self.val_data = self.split_data()\n","\n","    def encode(self, text):\n","        # Converts a string to an index column. `self.` is required to call other methods or arguments.\n","        return [self.ctoi[c] for c in text]\n","\n","    def decode(self, indices):\n","        return ''.join([self.itoc[i] for i in indices])\n","\n","    def split_data(self):\n","        split_index = int(0.9 * len(self.data))  # A split point to make 90% of the data for training.\n","        return self.data[:split_index], self.data[split_index:]\n","\n","    def get_batch(self, split):\n","        data = self.train_data if split == 'train' else self.val_data\n","        start_indices = torch.randint(len(data) - self.config.input_sequence_length, (self.config.batch_size,)) # Generate extract start index\n","\n","        input_sequences = torch.stack([\n","            data[start_index:start_index + self.config.input_sequence_length]\n","            for start_index in start_indices\n","        ])\n","        target_sequences = torch.stack([\n","            data[start_index + 1:start_index + self.config.input_sequence_length + 1]\n","            for start_index in start_indices\n","        ])\n","        return input_sequences.to(self.config.device_type), target_sequences.to(self.config.device_type)\n","\n","\n","class TokenEmbedding(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super().__init__()\n","        # Define an embedded table for vocabulary number x number of embedding dimensions\n","        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n","\n","    def embed(self, input_indices):\n","        # Get the embedded vector corresponding to the input index\n","        return self.token_embedding_table.forward(input_indices)\n","\n","class PositionEmbedding(nn.Module):\n","    def __init__(self, input_sequence_length = 8, embedding_dim = 8):\n","        super().__init__()\n","        # Location embedding layer\n","        self.position_embedding_layer = nn.Embedding(input_sequence_length, embedding_dim)\n","\n","    def forward(self, input_indices):\n","        # The shape of the input tensor input_indices: [batch size, sequence length].\n","        sequence_length = input_indices.shape[1]\n","\n","        # Create a position index according to the sequence length (e.g. [0, 1, 2, ..., sequence_length-1])\n","        position_indices = torch.arange(sequence_length, device=input_indices.device)\n","\n","        # Get the embedded vector for the position index\n","        position_embeddings = self.position_embedding_layer.forward(position_indices)\n","\n","        return position_embeddings\n","\n","class EmbeddingModule(nn.Module):\n","    def __init__(self, vocab_size, config):\n","        super().__init__()\n","        # Embedded layer for each token\n","        self.token_embedding_layer = TokenEmbedding(vocab_size = vocab_size, embedding_dim = config.embedding_dim)  # Word embedding layer\n","        self.position_embedding_layer = PositionEmbedding(input_sequence_length = config.input_sequence_length, embedding_dim = config.embedding_dim)  # Embed location information\n","\n","    def forward(self, input_indices):\n","        # Get token embedding\n","        token_embeddings = self.token_embedding_layer.embed(input_indices)\n","\n","        # Get location embedding\n","        position_embeddings = self.position_embedding_layer.forward(input_indices)\n","\n","        # Adding token embedding and position embedding\n","        embeddings = position_embeddings + token_embeddings\n","        return embeddings\n","\n","class LayerNorm(nn.Module):  # Inherit nn.Module here\n","    def __init__(self, token_length, eps=1e-5, norm_dim=-1):\n","        super().__init__()\n","        self.eps = eps\n","        self.norm_dim = norm_dim\n","\n","        # Register gamma and beta as nn.Parameter for use on both CPU and CUDA\n","        self.gamma = nn.Parameter(torch.ones(token_length))\n","        self.beta = nn.Parameter(torch.zeros(token_length))\n","\n","    def forward(self, x):\n","        mean = torch.mean(x, dim=self.norm_dim, keepdim=True)\n","        var = torch.var(x, dim=self.norm_dim, keepdim=True, unbiased=False)\n","        hat = (x - mean) / torch.sqrt(var + self.eps)\n","        output =  self.gamma * hat + self.beta\n","        return output\n","\n","\n","class AttentionHead(nn.Module):\n","    def __init__(self, head_size, config):\n","        super().__init__()\n","        self.key_fc= nn.Linear(config.embedding_dim, head_size, bias=False)\n","        self.query_fc = nn.Linear(config.embedding_dim, head_size, bias=False)\n","        self.value_fc = nn.Linear(config.embedding_dim, head_size, bias=False)\n","\n","        # Masks are created using the lower triangular matrix (maintaining the causality of self-attention)\n","        self.register_buffer('tril', torch.tril(torch.ones(config.input_sequence_length, config.input_sequence_length)))\n","\n","        # Dropout (deterministic version is defined separately)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","        self.head_size = head_size\n","\n","    def forward(self, input_tensor):\n","        B, T, C = input_tensor.shape  # Batch, Token Length, Embedding Channel\n","\n","        Key = self.key_fc.forward(input_tensor)     # (B, T, head_size)\n","        Query = self.query_fc.forward(input_tensor)   # (B, T, head_size)\n","        Value = self.value_fc.forward(input_tensor)   # (B, T, head_size)\n","\n","        # Calculating Attention score (QK^T) / sqrt(embedding_dim)\n","        attention_weights_before_mask = Query @ Key.transpose(-2, -1) * self.head_size**(-0.5)\n","\n","        # Mask applied\n","        mask = torch.triu(torch.ones(T, T), diagonal=1).to(input_tensor.device)\n","        masked_attention_weights = attention_weights_before_mask.masked_fill(mask == 1, float('-inf'))\n","\n","        # Softmax ‚Üí Dropout ‚Üí Weighted sum\n","        attention_weights = F.softmax(masked_attention_weights, dim=-1)\n","        attention_weights = self.dropout(attention_weights)\n","\n","        out = attention_weights @ Value  # (B, T, head_size)\n","        return out\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.num_attention_heads = config.num_attention_heads\n","        self.embedding_dim = config.embedding_dim\n","        self.head_size = int(self.embedding_dim / self.num_attention_heads)\n","\n","        # Manage multiple heads with ModuleList\n","        self.attention_heads = nn.ModuleList([\n","            AttentionHead(self.head_size, config)\n","            for _ in range(self.num_attention_heads)\n","        ])\n","\n","        # Linear layer for mixing the outputs of each head\n","        self.output_projection = nn.Linear(self.embedding_dim, self.embedding_dim)\n","\n","        # Dropouts for output\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","    def forward(self, input_tensor):\n","        # Get output from each head\n","        # List of (B, T, head_size)\n","        head_outputs_list = [head.forward(input_tensor) for head in self.attention_heads]\n","\n","        # Concatenate outputs of all heads ‚Üí (B, T, embedding_dim)\n","        concatenated = torch.cat(head_outputs_list, dim=-1)\n","\n","        # Mixing outputs with linear transformation\n","        projected = self.output_projection.forward(concatenated)\n","\n","        # Apply dropout to final output\n","        output = self.dropout.forward(projected)\n","\n","        return output\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(config.embedding_dim, config.hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(config.hidden_dim, config.embedding_dim),\n","            nn.Dropout(config.dropout_rate),\n","        )\n","\n","    def forward(self, input_tensor):\n","        return self.net(input_tensor)\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        # Each LayerNorm instance stores its own scaling parameters beta and gamma.\n","        self.layer_norm1 = nn.LayerNorm(config.embedding_dim)\n","        self.layer_norm2 = nn.LayerNorm(config.embedding_dim)\n","\n","        self.multihead_attention = MultiHeadAttention(config=config)\n","        self.feed_forward = FeedForward(config=config)\n","\n","    def forward(self, input_tensor):\n","        # forward method is omitted.\n","        normed_input = self.layer_norm1(input_tensor) # Apply Layer Norm to input\n","        attention_output = self.multihead_attention(normed_input) # Apply multi-head attention\n","        residual_attention = attention_output + input_tensor # add \"before! layernorm1\n","        normed_attention = self.layer_norm2(residual_attention) # Apply LayerNorm again to residual output\n","        feedforward_output = self.feed_forward(normed_attention) # Apply feedforward network (FFN)\n","        final_output = feedforward_output + residual_attention # add \"before\" layernorm2!\n","\n","        return final_output\n","\n","class VocabularyLogits(nn.Module):\n","    def __init__(self, vocab_size, config):\n","        super().__init__()\n","        # Layer normalization\n","        self.output_norm = nn.LayerNorm(config.embedding_dim)\n","        # Projection on vocabulary size\n","        self.vocab_projection = nn.Linear(config.embedding_dim, vocab_size)\n","\n","    def forward(self, transformer_block_output):\n","        # Apply Layer normalization to the output from the Transformer block.\n","        normalized_output = self.output_norm.forward(transformer_block_output)  # (B, T, C)\n","\n","        # Projects input scores into the vocabulary-size dimension using a linear layer.\n","        vocab_logits = self.vocab_projection.forward(normalized_output)  # (B, T, V)\n","\n","        return vocab_logits\n","\n","class BigramLanguageModel(nn.Module):\n","    def __init__(self, vocab_size, config):\n","        super().__init__()\n","        self.config = config  # It is also used when generating, so keep it.\n","        self.embedding = EmbeddingModule(vocab_size, config=config)\n","        self.blocks = nn.Sequential(*[TransformerBlock(config=config) for _ in range(config.layer_count)])\n","        self.vocab_projection = VocabularyLogits(vocab_size=vocab_size, config=config)\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    # Generate text\n","    def generate(self, input_indices, max_new_tokens):\n","        # Generate only the specified number of tokens `max_new_tokens`\n","        for _ in range(max_new_tokens):\n","            input_conditioned = input_indices[:, -self.config.input_sequence_length:] # Clip input\n","\n","            # Forward pass returns `(likelihood, loss)`‚Äîstore only the `likelihood` as `logits`.\n","            logits, _ = self.forward(input_conditioned, target_indices=None)\n","            last_logits = logits[:, -1, :] # Extract logit for the last token position\n","            probs = F.softmax(last_logits, dim=-1) # Convert likelihood to probability with Softmax\n","\n","            # Sample the next token\n","            next_token = torch.multinomial(probs, num_samples=1)\n","\n","            # Consolidate new tokens, update `input_indices`.\n","            input_indices = torch.cat((input_indices, next_token), dim=1)\n","\n","        # return final `input_indices`. The length is original `input_indices` + `max_new_tokens`\n","        return input_indices\n","\n","    # Calculate likelihood and loss\n","    def forward(self, input_indices, target_indices):\n","        embeddings = self.embedding(input_indices)\n","        blocks_output = self.blocks(embeddings)\n","        logits = self.vocab_projection(blocks_output)\n","\n","        # During inference there‚Äôs no target, so loss is `None`\n","        # ‚Äîonly the likelihood (logits) is returned.\n","        if target_indices is None:\n","            return logits, None\n","\n","        batch_size, token_len, vocab_size = logits.shape\n","        logits = logits.view(batch_size * token_len, vocab_size)\n","        targets = target_indices.view(batch_size * token_len)\n","        loss = self.criterion(logits, targets)\n","\n","        return logits, loss"]},{"cell_type":"markdown","metadata":{"id":"J0vcUT2kwwR5"},"source":["**Replace DeterministicDropout with nn.Dropout**\n","```python\n","AttentionHead: dropout = nn.Dropout(config.dropout_rate)\n","MultiHeadAttention: dropout = nn.Dropout(config.dropout_rate)\n","FeedForward:  nn.Dropout(config.dropout_rate)\n","```\n","<label><input type=\"checkbox\"> Done</label>"]},{"cell_type":"markdown","metadata":{"id":"QAqEd0cywV56"},"source":["**`Chapter 12: Trainer Class: Section 2: Class Summary`** <label><input type=\"checkbox\"> Mark as Done</label>"]},{"cell_type":"markdown","metadata":{"id":"2IGG8rZJT9fx"},"source":["### **Section 3: Training and Inference**"]},{"cell_type":"markdown","metadata":{"id":"8pdT24q93vOX"},"source":["So far, the embedding dimension was 8, with 2 attention heads, and 16-dim hidden layers in FeedForward Network.<br>\n","That‚Äôs too limited for good expression.<br>\n","Now, set the embedding dimension to 64, with 4 attention heads and 256-dim hidden layers in FeedForward Network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ooeKTSmYOdn","outputId":"505d4f44-f337-48eb-dde3-e5e4198a9b7a","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Each setting for the ModelConfig class:\n","Batch size: 16\n","Input sequence length: 32\n","Total training steps: 5000\n","Evaluation frequency (in steps): 100\n","Learning rate: 0.001\n","Device in use: cuda\n","Number of evaluation loops: 10\n","Embedding vector dimension: 64\n","Hidden layer dimension of the feedforward network: 256\n","Number of attention heads: 4\n","Number of model layers: 4\n","Dropout rate: 0.1\n","Random seed value: 1337\n"]}],"source":["# Configuration class that stores model settings\n","class ModelConfig:\n","    batch_size = 16  # Number of data to process at a time (batch size)\n","    input_sequence_length = 32  # Length of input data (sequence length)\n","    total_training_steps = 5000  # Maximum number of training (number of steps)\n","    evaluation_frequency = 100  # Frequency of evaluation of model performance\n","    learning_rate = 0.001  # Learning rate\n","    device_type = 'cuda' if torch.cuda.is_available() else 'cpu'  # Device to use (GPU or CPU)\n","    evaluation_loops = 10  # Number of repetitions during evaluation\n","    embedding_dim = 64  # Embedded layer size (number of dimensions of feature vector)\n","    hidden_dim = 256\n","    num_attention_heads = 4  # Note Mechanism Head Number\n","    layer_count = 4  # Number of layers in the model\n","    dropout_rate = 0.1  # Dropout probability\n","    random_seed_value = 1337  # Random number seeds for reproducibility\n","\n","# Verify your settings\n","config = ModelConfig()\n","\n","print(\"Each setting for the ModelConfig class:\")\n","print(f\"Batch size: {config.batch_size}\")\n","print(f\"Input sequence length: {config.input_sequence_length}\")\n","print(f\"Total training steps: {config.total_training_steps}\")\n","print(f\"Evaluation frequency (in steps): {config.evaluation_frequency}\")\n","print(f\"Learning rate: {config.learning_rate}\")\n","print(f\"Device in use: {config.device_type}\")\n","print(f\"Number of evaluation loops: {config.evaluation_loops}\")\n","print(f\"Embedding vector dimension: {config.embedding_dim}\")\n","print(f\"Hidden layer dimension of the feedforward network: {config.hidden_dim}\")\n","print(f\"Number of attention heads: {config.num_attention_heads}\")\n","print(f\"Number of model layers: {config.layer_count}\")\n","print(f\"Dropout rate: {config.dropout_rate}\")\n","print(f\"Random seed value: {config.random_seed_value}\")"]},{"cell_type":"markdown","metadata":{"id":"DwvOpyQPm5Jl"},"source":["**`Check Point`**\n","<label><input type=\"checkbox\"> Make sure each setting in the Config class shows correctly<br></label>\n","- Batch size: 16<br>\n","- Block size: 32<br>\n","- Max iterations: 5000<br>\n","- Evaluation interval: 100<br>\n","- Learning rate: 0.001<br>\n","- Device used: cuda or cpu<br>\n","- Number of evaluation iterations: 10<br>\n","- Embedding layer dimension: 64<br>\n","- Feedforward hidden dimension: 256<br>\n","- Number of attention heads: 4<br>\n","- Number of model layers: 4<br>\n","- Dropout rate: 0.1<br>\n","- Seed value: 1337<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EctA8L49Tsc0","outputId":"d7dfe830-e4bb-4f40-8b15-8cda7e56de12","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x782b68401490>"]},"metadata":{},"execution_count":6}],"source":["# Load configuration and set seed\n","config = ModelConfig()\n","torch.manual_seed(config.random_seed_value)  # Set random number seeds to ensure reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXBa9z1ogsUw"},"outputs":[],"source":["# Load data\n","with open(\"input.txt\", 'r', encoding = 'utf-8') as f:\n","    text_data = f.read()\n","data_loader = DataLoader(text_data, config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtyVxmwiTotc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c124075e-6a3d-413a-f0f9-ef56415b2dd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.209729 M parameters\n"]}],"source":["# Initialize model and optimizer\n","model = BigramLanguageModel(vocab_size = data_loader.vocab_size, config = config).to(config.device_type)  # Specify the device you use\n","optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n","\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"]},{"cell_type":"markdown","source":["In comparison, GPT2-Small model has 117M (or 124M) parameters."],"metadata":{"id":"mysHV8yjE7P6"}},{"cell_type":"markdown","metadata":{"id":"lx-g4ndFubat"},"source":["As a test, try generating at the pre-training stage."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpnodhC5ubat","outputId":"77b04870-2352-4921-fbea-3ad7f62188ef","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Let's henvIeuW;JcijkeLWfUaUJW;VcE!Pf;ocFF&oNhq$eQLJOOEFWXgjNhhqv;Es\n","iowD&ZqK&CgwN'Pq$mFHGjX.esumfonqUzgrN?pNVvN!Iebiqfs!EuOt3Zw?Bjx$oYk-wXmvevRibdVde!eJgRLKasNnge?DEYpK! 'scfoOl!Ebe$iol$-UpfXGKtewgLMsO!?fX?&D?;-$zBR.SudGdOo.&co\n","zvzNqQriRR'QbHbs'QqXghiHWJwLUEZE&pNz\n","T'Rk!ZgbN?tmE.uJaekBK?Oh&n&Um,LDqc'omcC&Z;xpZGipgRQeN$y?VDbOvsN,$IcNhepTHJeWkzKdrf?roHm?dfwFUpwMVg;ei&$RCXTyowaFZhjVBm$3g33cAuh,K?UlAGcX;p!JUlNvvbIHG.3inUc.HjMCsyhnpwAKylbSHT'pXh3UNfO:mreo'VrL'cpe-NC,ntZAziOpKcpTOE.hs:Ck&z'LGJgyb3?p!3fI,OjzFHE\n"]}],"source":["text = \"Let's he\"  # prompt\n","initial_context = torch.tensor(data_loader.encode(text), dtype=torch.long)\n","# Add batch dimension with unsqueeze(0) (batch size=1)\n","initial_context_unsqueeze = initial_context.unsqueeze(0)\n","# ‚Üì The key here is to move it to the device you are using (CPU or GPU)!\n","initial_context_unsqueeze = initial_context_unsqueeze.to(config.device_type)\n","\n","# Start generating\n","generated_sequence_initial = model.generate(initial_context_unsqueeze, max_new_tokens=500)\n","print(data_loader.decode(generated_sequence_initial[0].tolist()))"]},{"cell_type":"markdown","metadata":{"id":"CYintz_Aubat"},"source":["Hmm, this is a computerüíª, huh?\n","\n","Alright then, let's train it and raise it up to babyüë∂!"]},{"cell_type":"markdown","metadata":{"id":"GK_3F336Eu6z"},"source":["**Finally, we‚Äôve reached the last part. Starting the [emotional BGM¬π](https://youtu.be/GqmAe0QfkjU?feature=shared).<br>The training will take about 2 to 4 minutes. Let‚Äôs soak in the moment.**\n","\n","---\n","\n","Content Reference:  \n","¬π **DooPiano**, ‚ÄúBTS (Î∞©ÌÉÑÏÜåÎÖÑÎã®) ‚Äì Î¥ÑÎÇ† (Spring Day) Piano & String Orchestra Version,‚Äù YouTube, 3:41, published ~8.2‚ÄØyears ago (circa 2017). Accessed July‚ÄØ8‚ÄØ2025.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"k6t0rayVFh2U"},"source":["**Please make sure to listen to the BGM! You‚Äôre guaranteed to be moved! Now, run the cell below to start training!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpagygD5TqHA","outputId":"87c4c733-8f0d-45da-fe25-cd8b2fd6d5a4","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["===TRAINING STARTED SUCCESSFULLY===\n","Step 0: Train Loss 2.3080, Validation Loss 2.3095\n","Step 100: Train Loss 2.2592, Validation Loss 2.2887\n","Step 200: Train Loss 2.2122, Validation Loss 2.2228\n","Step 300: Train Loss 2.1850, Validation Loss 2.1929\n","Step 400: Train Loss 2.1433, Validation Loss 2.1610\n","Step 500: Train Loss 2.0731, Validation Loss 2.1434\n","Step 600: Train Loss 2.0713, Validation Loss 2.1265\n","Step 700: Train Loss 2.0781, Validation Loss 2.0784\n","Step 800: Train Loss 2.0399, Validation Loss 2.1171\n","Step 900: Train Loss 1.9939, Validation Loss 2.0921\n","Step 1000: Train Loss 2.0463, Validation Loss 2.0800\n","Step 1100: Train Loss 2.0031, Validation Loss 2.0437\n","Step 1200: Train Loss 1.9710, Validation Loss 2.0322\n","Step 1300: Train Loss 1.8837, Validation Loss 2.0148\n","Step 1400: Train Loss 1.9525, Validation Loss 2.0299\n","Step 1500: Train Loss 1.9432, Validation Loss 2.0055\n","Step 1600: Train Loss 1.9284, Validation Loss 1.9633\n","Step 1700: Train Loss 1.9180, Validation Loss 1.9909\n","Step 1800: Train Loss 1.8999, Validation Loss 1.9350\n","Step 1900: Train Loss 1.8392, Validation Loss 1.9623\n","Step 2000: Train Loss 1.8845, Validation Loss 1.9732\n","Step 2100: Train Loss 1.8869, Validation Loss 1.9537\n","Step 2200: Train Loss 1.8619, Validation Loss 1.9648\n","Step 2300: Train Loss 1.8186, Validation Loss 1.9966\n","Step 2400: Train Loss 1.8383, Validation Loss 1.9590\n","Step 2500: Train Loss 1.8500, Validation Loss 1.9092\n","Step 2600: Train Loss 1.8149, Validation Loss 1.8838\n","Step 2700: Train Loss 1.8128, Validation Loss 1.9213\n","Step 2800: Train Loss 1.8046, Validation Loss 1.9234\n","Step 2900: Train Loss 1.8057, Validation Loss 1.9302\n","Step 3000: Train Loss 1.7888, Validation Loss 1.8733\n","Step 3100: Train Loss 1.7705, Validation Loss 1.9013\n","Step 3200: Train Loss 1.7730, Validation Loss 1.9126\n","Step 3300: Train Loss 1.8045, Validation Loss 1.9461\n","Step 3400: Train Loss 1.7554, Validation Loss 1.9231\n","Step 3500: Train Loss 1.7937, Validation Loss 1.8936\n","Step 3600: Train Loss 1.7712, Validation Loss 1.8767\n","Step 3700: Train Loss 1.7668, Validation Loss 1.8839\n","Step 3800: Train Loss 1.7718, Validation Loss 1.8480\n","Step 3900: Train Loss 1.7603, Validation Loss 1.8709\n","Step 4000: Train Loss 1.7487, Validation Loss 1.8557\n","Step 4100: Train Loss 1.7145, Validation Loss 1.8529\n","Step 4200: Train Loss 1.7628, Validation Loss 1.8611\n","Step 4300: Train Loss 1.6964, Validation Loss 1.8522\n","Step 4400: Train Loss 1.7155, Validation Loss 1.8929\n","Step 4500: Train Loss 1.7233, Validation Loss 1.8741\n","Step 4600: Train Loss 1.7043, Validation Loss 1.8837\n","Step 4700: Train Loss 1.6532, Validation Loss 1.8430\n","Step 4800: Train Loss 1.6850, Validation Loss 1.7992\n","Step 4900: Train Loss 1.7202, Validation Loss 1.8850\n","Step 4999: Train Loss 1.7031, Validation Loss 1.9120\n","Training DONE\n","Model and optimizer state saved to bigram_language_model.pth\n"]}],"source":["print(\"===TRAINING STARTED SUCCESSFULLY===\")\n","\n","# Train the model\n","trainer = Trainer(model, optimizer, data_loader, config)\n","trainer.train()\n","\n","# Saving the model\n","save_path = \"bigram_language_model.pth\"\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict()\n","}, save_path)\n","\n","print(\"Training DONE\")\n","print(f\"Model and optimizer state saved to {save_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiEiQNKci0xq","outputId":"76f971cc-7a84-4c47-c9a6-dac8152999fa","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["===== Model Loaded Successfully!=====\n"]}],"source":["# ---- Loading saved models and generating text -----\n","# Initialize new model and optimizer (same settings and class definition required)\n","loaded_model = BigramLanguageModel(vocab_size = data_loader.vocab_size, config = config).to(config.device_type)  # Specify the device you use\n","loaded_optimizer = torch.optim.AdamW(loaded_model.parameters(), lr=config.learning_rate)\n","\n","save_path = \"bigram_language_model.pth\"\n","checkpoint = torch.load(save_path, map_location=config.device_type)\n","loaded_model.load_state_dict(checkpoint['model_state_dict'])\n","loaded_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","print(\"===== Model Loaded Successfully!=====\")"]},{"cell_type":"markdown","source":["Set the model to evaluation mode. Disable Dropout before generation.\n","\n","If you forget, Dropout turns on. The output will be awful, so please be careful."],"metadata":{"id":"y2Pj_bPCEPPS"}},{"cell_type":"code","source":["loaded_model.eval()\n","print(\"===== Set to Evaluation mode, disabled Dropout. =====\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5yuLBig8EG91","outputId":"83214a4c-14a2-4649-d21a-e1e6755d5a73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["===== Set to Evaluation mode, disabled Dropout. =====\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaQb254DuA8X","outputId":"b6f93c3a-e5ce-47b6-b2d3-d61613389e32","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["encoded_prompt\n","Tensor Size: [8]\n","tensor([\n","         24.00,  43.00,  58.00,   5.00,  57.00,   1.00,  46.00,  43.00\n","       ])\n"]}],"source":["prompt = \"Let's he\"  # prompt\n","encoded_prompt = torch.tensor(data_loader.encode(prompt), dtype=torch.long)\n","print_formatted_tensor(\"encoded_prompt\", encoded_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8QoDkdBPuC2Q","outputId":"cf25d96f-7b3b-4fc3-bc7a-80252e47a5d4","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["encoded_prompt_unsqueeze\n","Tensor Size: [1, 8]\n","tensor([\n","         [ 24.00,  43.00,  58.00,   5.00,  57.00,   1.00,  46.00,  43.00]\n","       ])\n"]}],"source":["# Add batch dimension with unsqueeze(0) (batch size=1)\n","encoded_prompt_unsqueeze = encoded_prompt.unsqueeze(0)\n","print_formatted_tensor(\"encoded_prompt_unsqueeze\", encoded_prompt_unsqueeze)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dUOQgT3uEHx"},"outputs":[],"source":["# ‚Üì The key here is to move the prompt tensor to the device you are using (CPU or GPU)!\n","encoded_prompt_unsqueeze = encoded_prompt_unsqueeze.to(config.device_type)"]},{"cell_type":"markdown","metadata":{"id":"RdiKzDt6zXpP"},"source":["```python\n","Instance: loaded_model\n","Method: generate\n","Arguments: encoded_prompt_unsqueeze, max_new_tokens=1000\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4U58DsEfuF27","outputId":"41bb755e-a8bc-4109-b385-79ede0010b35","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Let's he weath to begiver thing thou are perpisency the gertater sing\n","To With beenes; no from to the chaul!\n","\n","JUTIO:\n","Were this not the carence your in singrard hath lond\n","Capus leet.\n","\n","BRUTUS:\n","Thee bust to me to speaks or his it,\n","\n","Deseetock untertay laid I wand for shing\n","that ittience, God nig for the from do it it.\n","\n","Pervore, I sethall thou breather,\n","By tyre agent man some thou my servery plaid be the spoble.\n","\n","GLOUCESTER:\n","Shallo, Say fauls I weick.\n","\n","DUCESTER MARGAREY:\n","My noble thear, an, and at to he's guend\n","For me full I say's be swould Gentleat haph till\n","Nor And being years their rlancues\n","The Last burssened to her be gry our sto stoot\n","Ands that nurse steep\n","Of aund will of her age man, tiSingment your boy's rings:\n","Your my and again and well I would theur ady\n","A samernon to do time'n sail thou lack your juty,\n","No? I coundertife thou have thee other awarthrough a dne'd I preaty\n","Waveichard him time if our good bowers highne:\n","Thears.\n","\n","Thidds; and I' weal theseity. Bollow Richions:\n","As City you?I heark,\n"]}],"source":["# Start generating\n","generated_sequence = loaded_model.generate(encoded_prompt_unsqueeze, max_new_tokens=1000) # TODO: Instance,\n","print(data_loader.decode(generated_sequence[0].tolist()))"]},{"cell_type":"markdown","metadata":{"id":"zcHLCQ6P4O-8"},"source":["Has it progressed up to a babyüë∂ level?\n","\n","This time with nanoGPT, it moved from a computerüíª to a babyüë∂.\n","\n","Next, GPT2 will step up from a babyüë∂ to a middle schoolerüßë!"]},{"cell_type":"markdown","metadata":{"id":"R52XY3hdxKUS"},"source":["**`Chapter 12: Trainer Class: Section 3: Training and Inference`** <label><input type=\"checkbox\"> Mark as Done</label>"]},{"cell_type":"markdown","metadata":{"id":"viJYZTXwxNhC"},"source":["**`Chapter 12: The Trainer Class`** <label><input type=\"checkbox\"> Mark as Done</label>"]},{"cell_type":"markdown","metadata":{"id":"L1dLHoKgxQwm"},"source":["**`nanoGPT`** <label><input type=\"checkbox\"> Done</label>"]},{"cell_type":"markdown","metadata":{"id":"ykz9QCl8ubau"},"source":["![„Çπ„ÇØ„É™„Éº„É≥„Ç∑„Éß„ÉÉ„Éà 2025-06-25 0.58.23.png](data:image/png;base64,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)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}